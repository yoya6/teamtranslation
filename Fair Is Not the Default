公平并不是默认的
作者：Josh Lovejoy
本文改编自同名TEDx讲座，由作者编辑和修改

在威尼斯圣马可广场一幅可爱的人群照片中（如下图所示），如果你近一点观察就会发现一些奇怪的事情。每个人的头部都大致沿着同一条地平线。很奇怪，世界上每个人的身高都不一样对吧？这种错觉发生是因为摄影师有着平均身高。从另外一个角度来说，不是拥有平均身高的人来拍照，世界看起来也就不那么统一了。

 
威尼斯圣马可广场的人群照片。突出显示地平线，以说明人群中的人员身高似乎相对统一。
来源：Nino Barbieri，Creative Commons。

我是Google的设计师，我的工作是利用人工智能提供支持的产品（AI人工智能是涵盖某些或全部决策自动化的系统总称）。在过去一年左右，我一直在致力于领导公司，使公平成为机器学习过程的核心组成部分--机器学习是指导计算机根据数据中心自动发现的模式和关系进行预测的科学。然而什么是公平？并且从个人的角度来说，公平归结为两个基本信念：所设计和意图广泛使用的产品不应该因为无法改变而失败;技术应该为所有人服务。
虽然这些看起来并不像是一个有争议的观点，但当我们看传统产品设计的历史时，大量事实证明这些设计并不适合所有人，反而与不言而喻、显而易见的默认值或现状一致。

设计默认值的问题

这里以我们在美国测试汽车安全的方式为例。直到2011年，使用女性身体假人进行前碰撞测试并不是强制性的要求。在此之前，女性在车祸中严重受伤的可能性高出47％。即使现在，碰撞测试假人只能代表女性身高和体重的第五百分位，并且三次正面碰撞测试中有两次不会将其放在驾驶座上。
 
20世纪60年代和70年代的几张"雪莉卡"。绰号"雪莉"是适用于所有用于颜色标准化底片的柯达模型，特别是只有长头发的白人女性。

在20世纪60-70年代，"雪莉卡"是美国摄影师进行色彩校准的标准。这个想法是在处理照片时，试图通过提供的色板准确渲染色彩，而不会弄乱雪莉的肤色，因而理论上未来的照片也会更加精确。但你可能已经发现了一些不妥之处，到现在为止雪莉卡仅象征长头发的白人女性。
因而并没有发生变化，有人认识到这个过程非常不具代表性，或者通过观察浅色和深色皮肤之间白平衡和曝光度的明显差异（如左下图）。相反，1995年，但是直到巧克力公司和木制家具制造商向柯达抱怨印刷广告中可见的棕色有限光谱之后，柯达黄金公司的消费市场发生了变化。

 
左边，20世纪70年代的照片显示了肤色较暗的人在电影表现上的差异。
右边，几乎同时在家具杂志上刊登同样平淡的棕色广告。来源（从左至右）：Walt Jabsco，知识共享，经典电影，知识共享。

在电影行业，我们开始看到机器学习如何帮助提高对代表性差距的认识。Googler Hartwig的同事Adam与Geena Davis在媒体性别研究所合作，分析过去三年100部最受欢迎电影的每一帧图像。他们用分类技术比较了男性和女性出现的频率，其结果是：男性的视听率几乎是女性的两倍。
那么这些差距从哪里来？汽车公司是否故意伤害女性？色彩平衡是否明显是种族主义的决定？电影导演，制片人和作家是否试图故意边缘化女性？我选择另一个答案。我相信这些是理性行为者做出他们认为是明显决定的行为，因为他们符合默认值。
让我们再看一个例子。康奈尔大学的研究在七十年代后期进行--自从1983年、1986年、 2000年、2015年--让参与者在观看与许多不同刺激物相互作用的婴儿视频之后，评估婴儿的情绪反应。一半的参与者被告知这个孩子是个男孩，而另一半被告知是个女孩。当孩子微笑或大笑时，参与者普遍认同婴儿的主导情绪是快乐的。但是当孩子与一些刺耳的东西相互作用时，像蜂鸣器或打开盒子即跳出一个奇异小人的玩具盒，就会发生分歧。如果一个参与者被告知这个孩子是个女孩，他们认为她的主要情绪是恐惧。但是如果他们被告知这个孩子是个男孩，参与者认为这种主要的情绪是愤怒。同样的孩子，同样的反应，不同的看法。

  
小孩性别的独特差异对我们对其情绪的解释有重大影响。

大多数人可能会同意，对待恐惧和愤怒的人的看法不同是非理性的，然而感知不是理性的行为。我们了解另一个人的情绪状态大多是参考和凭借生活经验和文化内涵，而且信息会以意想不到的方式传达。我们甚至可能会因为他们的手很冷而认为其缺少关爱或者宽容，而我们的看法几乎推动了机器学习的各个方面。

数据和人的判断

当机器学习过程可视化时，它通常被描绘成一个复杂的节点阵列，串联成一个足够令人印象深刻的决策图表（见下图）。让我们一点点解开。
  
一个典型的神经网络显示连接"神经元"层的交叉线，并导致最后一层单一神经元突出显示，用来表示预测输出。

想象一下，一位教师为她的学生编写阅读清单。她希望他们掌握每本书中以不同方式表达的概念。单纯地记住各种答案不会产生任何实践应用。相反，她希望她的学生能够自己发现主题和模式，以便将来可以更广泛地使用。
大多数机器学习的开始方式非常相似，通过收集和注释相关真实世界内容的示例（这称为"训练数据"）。然后将这些示例输入到一个模型中，确定哪些细节对于分配的预测任务最显着。作为这一过程的结果，可以对模型以前从未见过的事情进行预测，并且这些预测可用于排序，过滤，排名甚至生成内容。
将这个过程误认为客观或中立的管道很容易。数据输入产生相应结果，但人类的判断贯穿始终。花点时间考虑以下几点：
o选择数据的来源，以及为什么他们认为选择的案例具有代表性。
o定义什么决定了成功，此外，用什么评估指标来衡量模型是否按预期工作。
o人们受到结果的影响。
机器学习不是运输程序，而是一个反馈循环。作为技术人员，我们无法抵挡辅助仪器的系统影响。我们每天接触到的媒体，包括被排序、分类、过滤或生成的方式，影响我们做出的决定。并且我们从中汲取，下一次我们着手构建模型。
考虑到人类感知中的缺陷，以及本文中详细描述的那些不太理想的产品决策，人们经常跳到的结论之一是我们需要从机器学习过程中去除人为判断。"如果我们能够摆脱这些令人讨厌的人类情感，"世界都会变得高效。
我们来看另一个例子。2016年，一家名为Northpointe的公司开发了软件来预测被授予假释的被告可能再犯罪的可能性。在所有其他标准相同的情况下，种族本身的差异显著提高了黑人的危险评分，远超过白人危险评分，且经常增加一倍以上。该公司声称，他们没有将种族作为训练模型的特征，但不幸的是，这种数据的盲目性实际上使问题变得更糟。鉴于深度学习的性质以及模型所曝光的大量信息，所有这一切都将得到保证，自动解析种族代理（如收入或邮政编码，与美国种族高度相关的属性）。
下面一个特别的案例来自FaceApp，该应用程序允许用户采取自拍并打开"热度"过滤器，但它往往只是让人看起来更白（因为被禁用）。这个例子说明当一个模型受到倾斜数据的训练时会发生什么。其中，主要的社交媒体行为来自白人国家。
另一个案例去年初在科学界引起了不小的轰动。来自中国的两位研究人员声称已经调试了一个模型，仅使用头像照片就可以预测某人是否犯罪率接近90％。这有许多方法论问题。例如，他们的数据集中的案例数量相当有限，他们无法确定他们被标记为"非犯罪分子"的人是否犯过罪，而且从未被逮捕。但更深层次上，他们的方法假定有人是天生的罪犯。如果在这种情况下有任何发现模式，这不是和个人判断有关，而是那些正在做判断的人。
这里没有万能之法。公平关联个体独特的经验。因此，当我们开始为人类构建技术框架时，使用人类数据，我们需要计划如何以希望的形式来表达偏见。让自己脱离这一过程并相信中立技术是不够的，因此我建议不要使机器学习以人为中心，并且为了公平而进行干预。

一种尺寸不能适合所有

当我们运行焦点小组时，有人开玩笑的提到，在人工智能的背景下天网或终结者只有很短的时间。就好像我们会去睡觉和醒来一样，机器人会继续接管。事实正相反，这是我们昨天做出的决定，或者是塑造今天和未来。所以会把他们放在一起。
我们需要对自己说的话持怀疑态度，不只是在谈论科幻小说。我们告诉自己，公平受到了挑战。尽管使用率相同，黑美国人被拘捕的人数是美国白人对毒品相关费用的四倍。我们告诉自己，像SAT这样的测试反映学术能力，却并没有真正预测学术成就。我们告诉自己，个性化医疗适合每个人，但大多数实验参与者是白人和男性。我们告诉自己，"人群"永远是真相的可靠来源。
  
早期版本的AI动力游戏Quick，Draw！在识别某些鞋款方面表现不佳，因为早期用户比其他鞋类更像往常一样绘制像Chuck Taylor Converse（上图，左图）的草图，如高跟鞋（上图，右图）。

游戏"Quick, Draw!" 是一个很好的案例，通过Google的AI实验计划开发，说明为什么在测试产品时考虑多种观点至关重要。在游戏中，人们被要求画出像鞋子这样的日常用品来训练模型。大部分的早期用户都穿着看起来像运动鞋的鞋子，所以当更多的人与游戏互动时，模型不能识别任何不是运动鞋类型的鞋子。有了这些研究结果，团队能够故意寻找额外的训练实例来弥补代表性差距，并且性能显着提高。我们已经开源了Facets 可视化工具，以便更多人可以应用这种广角镜头来评估他们的数据中无用的一致性。
为公平而设计意味着在保持开放和好奇心的同时，面对世界的不平等状态。它首先提出问题：我是谁？我怎样才能更好地理解其他人的需求？

  
有许多独特的面部特征组合。简单的类别从不讲述整个故事。

有许多特性会在今天的科技行业被视为默认的。我是白人，我是一名温和的男人，有异性伴侣。但我也有一些超出常态的特质。我很高，从理论上讲也是盲人。而作为一个犹太人，我在一个文化少数群体中长大。
我们每个人都是复杂的，我们的特征并不能定义我们，但假装别人看不到是愚昧的。通过采取较不自然的形式来总结自己的特征，包括身体、社交、认知等方面。因而当在日常生活中调用特征与默认不匹配时，我们可以更好地像局外人一样的感知体验。对很多人来说，不会选择这种方式。
我们都有共同的愿望，要成为"我们"而不必承担别人的先入之见。通过评估价值和目标的假设，我们可以为讨论中的更多声音留出空间。希望在这个过程中，我们可以学习看到这个世界不再是二元的：像我这样的人（同意），和不像我的人（反对）。相反，将它看作是一个相互交织的世界，我们将差异作为更好的理解机会来庆祝。这样做，可以充分利用机器学习的潜力帮助我们找到对应模式，尤其是利益方面。这个形式是一个可以丰富人的联系，增强人的能力，为探索者服务的工具，可以帮助我们看到超出默认值外的特征。

有关如何采用以人为本的机器学习和人工智能设计方法的更多信息，请访问我们的完整集合02/15/2018


